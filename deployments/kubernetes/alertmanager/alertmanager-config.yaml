apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      # SMTP configuration
      smtp_smarthost: 'smtp.example.com:587'
      smtp_from: 'alertmanager@example.com'
      smtp_auth_username: 'alertmanager@example.com'
      smtp_auth_password_file: '/etc/alertmanager/secrets/smtp-password'
      smtp_require_tls: true
      # Slack configuration
      slack_api_url_file: '/etc/alertmanager/secrets/slack-webhook'

    # Templates
    templates:
      - '/etc/alertmanager/templates/*.tmpl'

    # Root routing configuration
    route:
      group_by: ['alertname', 'cluster', 'namespace', 'service']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      receiver: 'default'
      
      routes:
        # Critical alerts - immediate notification
        - match:
            severity: critical
          receiver: 'critical'
          group_wait: 10s
          group_interval: 1m
          repeat_interval: 1h
          continue: true
          
        # High priority alerts
        - match:
            severity: high
          receiver: 'high-priority'
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 2h
          
        # Warning alerts - grouped notifications
        - match:
            severity: warning
          receiver: 'warning'
          group_wait: 5m
          group_interval: 15m
          repeat_interval: 6h
          
        # Info alerts - low priority
        - match:
            severity: info
          receiver: 'info'
          group_wait: 10m
          group_interval: 30m
          repeat_interval: 12h
          
        # Namespace-specific routing
        - match:
            namespace: production
          receiver: 'production-team'
          routes:
            - match:
                severity: critical
              receiver: 'production-critical'
              
        # Service-specific routing
        - match_re:
            service: (database|storage)
          receiver: 'database-team'
          
        - match_re:
            service: (api|gateway|backend)
          receiver: 'backend-team'
          
        - match:
            service: frontend
          receiver: 'frontend-team'
          
        # Alert type specific routing
        - match:
            alertname: DeadMansSwitch
          receiver: 'health-check'
          repeat_interval: 30s
          
        - match_re:
            alertname: (.*Down$|.*Unavailable$)
          receiver: 'sre-team'
          group_wait: 10s

    # Receiver configurations
    receivers:
      - name: 'default'
        slack_configs:
          - channel: '#alerts'
            title: 'Alert: {{ .GroupLabels.alertname }}'
            text: '{{ template "slack.default.text" . }}'
            send_resolved: true
            
      - name: 'critical'
        email_configs:
          - to: 'oncall@example.com,critical-alerts@example.com'
            headers:
              Subject: '[CRITICAL] {{ .GroupLabels.alertname }} - {{ .GroupLabels.cluster }}'
            html: '{{ template "email.critical.html" . }}'
            text: '{{ template "email.critical.text" . }}'
            send_resolved: true
        slack_configs:
          - channel: '#critical-alerts'
            title: ':fire: CRITICAL: {{ .GroupLabels.alertname }}'
            text: '{{ template "slack.critical.text" . }}'
            color: 'danger'
            send_resolved: true
        webhook_configs:
          - url_file: '/etc/alertmanager/secrets/pagerduty-webhook'
            send_resolved: true
            
      - name: 'high-priority'
        email_configs:
          - to: 'alerts@example.com'
            headers:
              Subject: '[HIGH] {{ .GroupLabels.alertname }} - {{ .GroupLabels.namespace }}'
            html: '{{ template "email.high.html" . }}'
            send_resolved: true
        slack_configs:
          - channel: '#high-priority-alerts'
            title: ':warning: HIGH: {{ .GroupLabels.alertname }}'
            text: '{{ template "slack.high.text" . }}'
            color: 'warning'
            
      - name: 'warning'
        slack_configs:
          - channel: '#warnings'
            title: 'Warning: {{ .GroupLabels.alertname }}'
            text: '{{ template "slack.warning.text" . }}'
            color: 'warning'
            send_resolved: false
            
      - name: 'info'
        slack_configs:
          - channel: '#info-alerts'
            title: 'Info: {{ .GroupLabels.alertname }}'
            text: '{{ template "slack.info.text" . }}'
            send_resolved: false
            
      - name: 'production-team'
        email_configs:
          - to: 'production-team@example.com'
            headers:
              Subject: '[PROD] {{ .GroupLabels.alertname }}'
            html: '{{ template "email.production.html" . }}'
        slack_configs:
          - channel: '#production-alerts'
            title: 'Production Alert: {{ .GroupLabels.alertname }}'
            text: '{{ template "slack.production.text" . }}'
            
      - name: 'production-critical'
        email_configs:
          - to: 'production-oncall@example.com'
            headers:
              Subject: '[PROD-CRITICAL] {{ .GroupLabels.alertname }}'
        slack_configs:
          - channel: '#production-critical'
            color: 'danger'
        webhook_configs:
          - url_file: '/etc/alertmanager/secrets/prod-pagerduty-webhook'
            
      - name: 'database-team'
        email_configs:
          - to: 'database-team@example.com'
        slack_configs:
          - channel: '#database-alerts'
            
      - name: 'backend-team'
        email_configs:
          - to: 'backend-team@example.com'
        slack_configs:
          - channel: '#backend-alerts'
            
      - name: 'frontend-team'
        email_configs:
          - to: 'frontend-team@example.com'
        slack_configs:
          - channel: '#frontend-alerts'
            
      - name: 'sre-team'
        email_configs:
          - to: 'sre-team@example.com'
        slack_configs:
          - channel: '#sre-alerts'
            color: 'danger'
            
      - name: 'health-check'
        webhook_configs:
          - url: 'http://health-monitor.monitoring.svc.cluster.local/ping'
            send_resolved: false

    # Inhibition rules
    inhibit_rules:
      # Critical alerts inhibit warnings for same service
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        equal: ['alertname', 'cluster', 'namespace', 'service']
        
      # Service down inhibits other service alerts
      - source_match:
          alertname: 'ServiceDown'
        target_match_re:
          alertname: '(ServiceHighLatency|ServiceErrors)'
        equal: ['service', 'namespace']
        
      # Cluster-level issues inhibit node-specific alerts
      - source_match:
          alertname: 'ClusterUnreachable'
        target_match_re:
          alertname: 'Node.*'
        equal: ['cluster']
        
      # Database down inhibits connection pool alerts
      - source_match:
          alertname: 'DatabaseDown'
        target_match:
          alertname: 'DatabaseConnectionPoolExhausted'
        equal: ['database', 'namespace']

    # Mute time periods (optional)
    mute_time_intervals:
      - name: 'weekends'
        time_intervals:
          - weekdays: ['saturday', 'sunday']
      - name: 'off-hours'
        time_intervals:
          - times:
              - start_time: '00:00'
                end_time: '09:00'
              - start_time: '18:00'
                end_time: '24:00'