groups:
  - name: infrastructure_alerts
    interval: 30s
    rules:
      # Node alerts
      - alert: NodeDown
        expr: up{job="node-exporter"} == 0
        for: 5m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Node {{ $labels.instance }} is down"
          description: "Node {{ $labels.instance }} has been down for more than 5 minutes."
          runbook_url: "https://runbooks.company.com/infrastructure/node-down"
      
      - alert: HighNodeCPU
        expr: node:cpu_utilization > 0.85
        for: 10m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High CPU usage on node {{ $labels.instance }}"
          description: "Node {{ $labels.instance }} CPU usage is above 85% (current value: {{ $value | humanizePercentage }})"
          runbook_url: "https://runbooks.company.com/infrastructure/high-cpu"
      
      - alert: HighNodeMemory
        expr: node:memory_utilization > 0.85
        for: 10m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High memory usage on node {{ $labels.instance }}"
          description: "Node {{ $labels.instance }} memory usage is above 85% (current value: {{ $value | humanizePercentage }})"
          runbook_url: "https://runbooks.company.com/infrastructure/high-memory"
      
      - alert: NodeDiskSpaceLow
        expr: node:filesystem_free_percent < 15
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Low disk space on node {{ $labels.instance }}"
          description: "Node {{ $labels.instance }} has less than 15% disk space available on {{ $labels.mountpoint }} (current: {{ $value | humanize }}%)"
          runbook_url: "https://runbooks.company.com/infrastructure/low-disk-space"
      
      - alert: NodeDiskSpaceCritical
        expr: node:filesystem_free_percent < 5
        for: 5m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Critical disk space on node {{ $labels.instance }}"
          description: "Node {{ $labels.instance }} has less than 5% disk space available on {{ $labels.mountpoint }} (current: {{ $value | humanize }}%)"
          runbook_url: "https://runbooks.company.com/infrastructure/critical-disk-space"
      
      # Kubernetes cluster alerts
      - alert: KubernetesNodeNotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Kubernetes node {{ $labels.node }} is not ready"
          description: "Kubernetes node {{ $labels.node }} has been in NotReady state for more than 5 minutes."
          runbook_url: "https://runbooks.company.com/kubernetes/node-not-ready"
      
      - alert: KubernetesPodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: warning
          team: application
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container }} has restarted {{ $value }} times in the last 15 minutes."
          runbook_url: "https://runbooks.company.com/kubernetes/pod-crash-loop"
      
      - alert: KubernetesPodNotHealthy
        expr: sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"}) > 0
        for: 15m
        labels:
          severity: warning
          team: application
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is not healthy"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in {{ $labels.phase }} state for more than 15 minutes."
          runbook_url: "https://runbooks.company.com/kubernetes/pod-not-healthy"
      
      - alert: KubernetesDeploymentReplicasMismatch
        expr: |
          kube_deployment_spec_replicas != kube_deployment_status_replicas_available
        for: 15m
        labels:
          severity: warning
          team: application
        annotations:
          summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replica mismatch"
          description: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has {{ $value }} replicas available, expected {{ $labels.spec_replicas }}."
          runbook_url: "https://runbooks.company.com/kubernetes/deployment-replica-mismatch"

  - name: application_alerts
    interval: 30s
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: service:error_rate > 0.05
        for: 5m
        labels:
          severity: warning
          team: application
        annotations:
          summary: "High error rate for service {{ $labels.service }}"
          description: "Service {{ $labels.service }} in namespace {{ $labels.namespace }} has error rate above 5% (current: {{ $value | humanizePercentage }})"
          runbook_url: "https://runbooks.company.com/application/high-error-rate"
      
      - alert: CriticalErrorRate
        expr: service:error_rate > 0.10
        for: 5m
        labels:
          severity: critical
          team: application
        annotations:
          summary: "Critical error rate for service {{ $labels.service }}"
          description: "Service {{ $labels.service }} in namespace {{ $labels.namespace }} has error rate above 10% (current: {{ $value | humanizePercentage }})"
          runbook_url: "https://runbooks.company.com/application/critical-error-rate"
      
      # Slow response time
      - alert: SlowResponseTime
        expr: service:request_duration_seconds:p95 > 1.0
        for: 10m
        labels:
          severity: warning
          team: application
        annotations:
          summary: "Slow response time for service {{ $labels.service }}"
          description: "Service {{ $labels.service }} in namespace {{ $labels.namespace }} p95 latency is above 1s (current: {{ $value | humanizeDuration }})"
          runbook_url: "https://runbooks.company.com/application/slow-response-time"
      
      - alert: VerySlowResponseTime
        expr: service:request_duration_seconds:p95 > 5.0
        for: 5m
        labels:
          severity: critical
          team: application
        annotations:
          summary: "Very slow response time for service {{ $labels.service }}"
          description: "Service {{ $labels.service }} in namespace {{ $labels.namespace }} p95 latency is above 5s (current: {{ $value | humanizeDuration }})"
          runbook_url: "https://runbooks.company.com/application/very-slow-response-time"
      
      # Traffic anomalies
      - alert: TrafficDropDetected
        expr: |
          (service:request_rate < 0.5 * avg_over_time(service:request_rate[1h] offset 1d))
          and (avg_over_time(service:request_rate[1h] offset 1d) > 1)
        for: 15m
        labels:
          severity: warning
          team: application
        annotations:
          summary: "Traffic drop detected for service {{ $labels.service }}"
          description: "Service {{ $labels.service }} in namespace {{ $labels.namespace }} is receiving less than 50% of normal traffic"
          runbook_url: "https://runbooks.company.com/application/traffic-drop"
      
      - alert: TrafficSpikeDetected
        expr: |
          service:request_rate > 2.0 * avg_over_time(service:request_rate[1h] offset 1d)
          and service:request_rate > 100
        for: 10m
        labels:
          severity: warning
          team: application
        annotations:
          summary: "Traffic spike detected for service {{ $labels.service }}"
          description: "Service {{ $labels.service }} in namespace {{ $labels.namespace }} is receiving more than 2x normal traffic"
          runbook_url: "https://runbooks.company.com/application/traffic-spike"

  - name: service_mesh_alerts
    interval: 30s
    rules:
      # Circuit breaker alerts
      - alert: CircuitBreakerOpen
        expr: |
          sum by (destination_service_name, destination_service_namespace, source_workload) (
            rate(istio_tcp_connections_opened_total[5m])
          ) == 0
          and 
          sum by (destination_service_name, destination_service_namespace, source_workload) (
            rate(istio_request_duration_milliseconds_count[5m])
          ) > 0
        for: 5m
        labels:
          severity: warning
          team: application
        annotations:
          summary: "Circuit breaker open for {{ $labels.destination_service_name }}"
          description: "Circuit breaker is open from {{ $labels.source_workload }} to {{ $labels.destination_service_namespace }}/{{ $labels.destination_service_name }}"
          runbook_url: "https://runbooks.company.com/service-mesh/circuit-breaker-open"
      
      # High retry rate
      - alert: HighRetryRate
        expr: |
          sum by (destination_service_name, destination_service_namespace) (
            rate(istio_request_duration_milliseconds_count{response_flags=~".*retry.*"}[5m])
          ) /
          sum by (destination_service_name, destination_service_namespace) (
            rate(istio_request_duration_milliseconds_count[5m])
          ) > 0.10
        for: 10m
        labels:
          severity: warning
          team: application
        annotations:
          summary: "High retry rate for service {{ $labels.destination_service_name }}"
          description: "Service {{ $labels.destination_service_namespace }}/{{ $labels.destination_service_name }} has retry rate above 10% (current: {{ $value | humanizePercentage }})"
          runbook_url: "https://runbooks.company.com/service-mesh/high-retry-rate"
      
      # Service mesh latency
      - alert: ServiceMeshHighLatency
        expr: mesh:request_duration_p99 > 1000
        for: 10m
        labels:
          severity: warning
          team: application
        annotations:
          summary: "High service mesh latency for {{ $labels.destination_service_name }}"
          description: "Service mesh p99 latency from {{ $labels.source_workload }} to {{ $labels.destination_service_namespace }}/{{ $labels.destination_service_name }} is above 1s (current: {{ $value }}ms)"
          runbook_url: "https://runbooks.company.com/service-mesh/high-latency"
      
      # Service mesh errors
      - alert: ServiceMeshHighErrorRate
        expr: |
          1 - mesh:request_success_rate < 0.95
        for: 5m
        labels:
          severity: warning
          team: application
        annotations:
          summary: "High service mesh error rate for {{ $labels.destination_service_name }}"
          description: "Service mesh error rate from {{ $labels.source_workload }} to {{ $labels.destination_service_namespace }}/{{ $labels.destination_service_name }} is above 5%"
          runbook_url: "https://runbooks.company.com/service-mesh/high-error-rate"

  - name: slo_alerts
    interval: 30s
    rules:
      # Multi-window multi-burn-rate alerts (following Google SRE book)
      - alert: SLOErrorBudgetBurnRateHigh
        expr: |
          (
            slo:error_rate_1h > (14.4 * 0.001)
            and
            slo:error_rate_6h > (14.4 * 0.001)
          )
          or
          (
            slo:error_rate_6h > (6 * 0.001)
            and
            slo:error_rate_24h > (6 * 0.001)
          )
        for: 2m
        labels:
          severity: warning
          team: application
          slo: "availability"
        annotations:
          summary: "High error budget burn rate for {{ $labels.service }}"
          description: "Service {{ $labels.namespace }}/{{ $labels.service }} is burning error budget at high rate. 1h error rate: {{ $labels.error_rate_1h | humanizePercentage }}, 6h error rate: {{ $labels.error_rate_6h | humanizePercentage }}"
          runbook_url: "https://runbooks.company.com/slo/high-burn-rate"
      
      - alert: SLOErrorBudgetBurnRateCritical
        expr: |
          (
            slo:error_rate_1h > (36 * 0.001)
            and
            slo:error_rate_6h > (36 * 0.001)
          )
        for: 2m
        labels:
          severity: critical
          team: application
          slo: "availability"
        annotations:
          summary: "Critical error budget burn rate for {{ $labels.service }}"
          description: "Service {{ $labels.namespace }}/{{ $labels.service }} is burning error budget at critical rate. 1h error rate: {{ $labels.error_rate_1h | humanizePercentage }}"
          runbook_url: "https://runbooks.company.com/slo/critical-burn-rate"
      
      - alert: SLOLatencyBudgetBurn
        expr: |
          slo:latency_target_rate < 0.95
        for: 10m
        labels:
          severity: warning
          team: application
          slo: "latency"
        annotations:
          summary: "Latency SLO violation for {{ $labels.service }}"
          description: "Service {{ $labels.namespace }}/{{ $labels.service }} is not meeting latency SLO. Only {{ $value | humanizePercentage }} of requests are under 1s threshold."
          runbook_url: "https://runbooks.company.com/slo/latency-violation"
      
      - alert: SLOAvailabilityRisk
        expr: |
          slo:availability_24h < 0.995
        for: 15m
        labels:
          severity: warning
          team: application
          slo: "availability"
        annotations:
          summary: "24h availability below target for {{ $labels.service }}"
          description: "Service {{ $labels.namespace }}/{{ $labels.service }} 24h availability is {{ $value | humanizePercentage }}, below 99.5% target"
          runbook_url: "https://runbooks.company.com/slo/availability-risk"

  - name: resource_alerts
    interval: 30s
    rules:
      # Container resource alerts
      - alert: ContainerHighCPU
        expr: |
          sum by (namespace, pod, container) (rate(container_cpu_usage_seconds_total[5m])) 
          / sum by (namespace, pod, container) (container_spec_cpu_quota / container_spec_cpu_period) > 0.9
        for: 10m
        labels:
          severity: warning
          team: application
        annotations:
          summary: "Container {{ $labels.container }} has high CPU usage"
          description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} is using more than 90% of CPU limit"
          runbook_url: "https://runbooks.company.com/container/high-cpu"
      
      - alert: ContainerHighMemory
        expr: |
          container_memory_working_set_bytes 
          / container_spec_memory_limit_bytes > 0.9
        for: 10m
        labels:
          severity: warning
          team: application
        annotations:
          summary: "Container {{ $labels.container }} has high memory usage"
          description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} is using more than 90% of memory limit"
          runbook_url: "https://runbooks.company.com/container/high-memory"
      
      - alert: ContainerOOMKilled
        expr: |
          kube_pod_container_status_last_terminated_reason{reason="OOMKilled"} == 1
        for: 1m
        labels:
          severity: warning
          team: application
        annotations:
          summary: "Container {{ $labels.container }} was OOM killed"
          description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} was killed due to out of memory"
          runbook_url: "https://runbooks.company.com/container/oom-killed"